<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ’­&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>References&nbsp;|&nbsp;DignoLog</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="References">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“š&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ’­&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="references-626569.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“š&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>References</span>
        </div>
      </a>
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about-me-a7c17b.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“§&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About me</span>
        </div>
      </a>
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“š&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">References</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Thu, Apr 7, 2022</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--gray">
            <a href="tag/Bib.html">Bib</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/62656911b88743e8840b2545c9d4afe6" class="PageRoot"><div id="https://www.notion.so/3bd254964e2c43e9bb7869cf21f0780a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">[ ABZ-2010 ]	Optimizing fixed-size stochastic controllers for POMDPs and Dec-POMDPs, C. Amato, D. S. Bernstein, S. Zilberstein (2010)
[ ACZ-2007 ]	Bounded Dynamic Programming for Dec-POMDPs, A. Carlin, C. Amato, S. Zilberstein (2017)
[ AD-2019 ]	Feudal MA Hierarchies for Cooperative EL, P. Dayan, S. Ahilan (2019)
[ ADZ-2009 ]	Incremental Policy Generation for Finite-Horizon Dec-POMDPs, C. Amato, J. S. Dibangoye, S. Zilberstein (2009)
[ AJWT-2019 ]	Control Theory Meets POMDPs: A Hybrid System Approach, B. Wu, M. Ahmadi, N. Jansen, U. Topcu (2019)
[ AKK-2014 ]	Planning with Macro-Actions in Dec-POMDPs, C. Amato, G. D. Konidaris, L. P. Kaelbling (2014)
[ AKLM-2020 ]	On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift, A. Agrawal, G. Mahajan, J. Lee, S. Kakade (2020)
[ AL-2008 ]	A MARL algorithm with non-linear dynamics, S. Adballah, V. Lesser (2008)
[ ALA-2018 ]	RL in ROMDPs using Spectral Methods, A. Anandkumar, A. Lazaric, K. Azizzadenesheli (2018)
[ AS-2018 ]	Autonomous agents modelling other agents: a survey, P. Stone, S. V. Albrech (2018)
[ ASBA-2019 ]	Safe Policy Synthesis in Multi-Agent POMDPs via Discrete-Time Barrier Functions, A. D. Ames, A. Singletary, J. W. Burdick, M. Ahmadi (2019)
[ ASBA-2020 ]	Barrier Functions for MA-POMDPs with DTL Specifications, A. D. Ames, A. Singletary, J. W. Burdick, M. Ahmadi (2020)
[ AWSplus-2021 ]	A MARL framework for intelligent manufacturing with autonomous mobile robots, A. Agrawal, C. McComb, M. Deshpande, S. J. Won, T. Sharma (2021)
[ BATT-2021 ]	GoSafe: Globally Optimal Safe Robot Learning, A. Marco, D. Baumann, M. Turchetta, S. Trimpe (2021)
[ BBCplus-2019 ]	Dota2 with large scale Deep-RL, B. Chan, C. Berner, G. Brockman (2019)
[ BBS-2020 ]	Cooperative and Stochastic Multi-Player Multi-Armed Bandit: Optimal Regret With Neither Communication Nor Collisions, M. Selke, S. Bubeck, T. Budnizinski (2020)
[ BC-1998 ]	The dynamics of RL in cooperative MAS, C. Boutilier, C. Claus (1998)
[ BD-1995 ]	RL methods for continuous-time MDPs, M. O. Duff, S. J. Bradtke (1995)
[ BDM-2017 ]	A Distributional Perspective on RL, M. Bellemare, R. Munos, W. Dabney (2017)
[ BDMplus-2019 ]	Cooperative MA policy gradient, F. Pereyron, G. Bono, J. S. Dibangoye, L. Matignon (2019)
[ BKBGB-2020 ]	Multiagent Rollout and Policy Iteration for POMDP with Application to Multi-Robot Repair Problems, D. Bertsekas, S. Badyal, S. Bhattacharya, S. Gil, S. Kailas (2020)
[ BKMplus-2020 ]	Emergent tool use from MA autocurricula, B. Baker, I. Kanitcheider, T. Markov (2020)
[ BLKplus-2012 ]	Sample Bounded Distributed RL for Dec-POMDPs, B. Banerjee, J. Lyle, L. Kraemer (2012)
[ BLM-2007 ]	Learning in Complex Environments through Multiple Adaptive Partitions, A. Bonarini, A. Lazaric, M. Restelli (2007)
[ BM-2019 ]	Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits, Y. Bar-On, Y. Mansour (2019)
[ BRHNP-2019 ]	Promoting coordination through policy regularisation in MARL, F. G. Harvey, J. Roy, P. Barde (2019)
[ BRMplus-2018 ]	The Mechanics of n-Player Differentiable Games, D. Balduzzi, J. Foerster, J. Martens, K. Tuyls, S. Racaniere, T. Graepel (2018)
[ BSKS-2017 ]	Making friends on the fly, A. Rosenfeld, P. Stone, S. Barret, S. Kraus (2017)
[ BV-2002 ]	MA learning using a variable learning rate, M. Bowling, M. Veloso (2002)
[ CBK-2020 ]	Efficient Model-Based RL through Optimistic Policy Search and Planning, A. Krause, F. Berkenkamp, S. Curi (2020)
[ CCK-2020 ]	MARL for networked system control, S. Chinchali, S. Katti, T. Chu (2020)
[ CDB-2016 ]	RL in PO MA Settings: MC Exploring Policies with PAC Bounds, B. Banarjee, P. Doshi, R. Ceren (2016)
[ CJJ-2021 ]	 Online Learning for Cooperative Multi-Player Multi-Armed
Bandits, M. Jahromi, R. Jain, W. Chang (2021)
[ CJJMST-2021 ]	Robust Finite-State Controllers for Uncertain POMDPs, A. Marandi, M. Cubuktepe, M. Suilen, N. Jansen, S. Junges, U. Topcu (2021)
[ COSW-2019 ]	The representational capacity of action-value network for MARL, F. A. Oliehoek, J. Castellini, R. Savani, S. Whiteson (2019)
[ CS-2006 ]	AWESOME: A general MA learning algorithm that converges in self-play and learns a best response against stationary opponents, T. Sandholm, V. Conitzer (2006)
[ CSMO-2013 ]	Decentralized multi-robot cooperation with auctioned POMDPs, A. Ollero, J. S. Capitan, L. Merino, M. T. J. Spaan (2013)
[ CWRplus-2020 ]	Option-critic in cooperative MAS, A. Lupu, D. Precup, J. Chakravorty, J. Roy, M. C. Boisvert, N. Ward, S. Basu (2020)
[ CYS-2019 ]	 MA fully decentralized value function learning with
linear convergence rates, A. H. Sayed, K. Yuan, L. Cassano (2019)
[ CZ-2008 ]	Value-based observation compression for Dec-POMDPs, A. Carlin, S. Zilberstein (2008)
[ CZWplus-2018 ]	Factorized Q-learning for large-scale MAs, M. Zhou, R. Y. Chen, W. Shang, Y. Wen (2018)
[ DABC-2016 ]	Optimally solving Dec-POMDPs as continuous-state MDPs, C. Amato, F. Charpillet, J. S. Dibangoye, O. Buffet (2016)
[ DB-2018 ]	Learning to act in Dec-POMDPs, J. S. Dibangoye, O. Buffet (2018)
[ DC-2017 ]	Accelerating MARL through TL, A. H. R. Costa, F. Da Silva (2017)
[ DC-2019 ]	A Survey on transfer learning for MARL systems, A. H. R. Costa, F. Da Silva (2019)
[ DGC-2017 ]	Simultaneously learning and advising in MARL, A. H. R. Costa, F. Da Silva, R. Glatt (2017)
[ DI-2009 ]	Solving Stochastic Games, C. L. Isbell, L. M. Dermed (2009)
[ DKT-2017 ]	Fully decentralized policies for MAS: An Information theoretic approach, C. Tomlin, D. F. Keil, R. Dobbe (2017)
[ DL-1995 ]	Decomposing techniques for planning in stochastic domains , S. H. Lin, T. Dean (1995)
[ DMC-2009 ]	Point-based incremental pruning heuristic for solving finite-horizon Dec-POMDPs, A. Mouaddib, B. Chaib-draa, J. S. Dibangoye (2009)
[ DMR-2019 ]	Finite-time performance for distributed TD learning with linear function approximation, J. Romberg, S. T. Maguluri, T. T. Doan (2019)
[ DNP-2012 ]	Hierarchical Relative Entropy Policy Search, C. Daniel, G. Neumann, J. Peters (2012)
[ DRBM-2017 ]	Distributional RL with Quantile Regression, M. Bellemare, M. Rowland, R. Munos, W. Dabney (2017)
[ FCAplus-2018 ]	Learning with Opponent-Learning Awareness, I. Mordatch, J. Foerster, M. Al-Shedivat, P.  Abbeel, R. Y. Chen, S. Whiteson (2018)
[ FFAplus-2018 ]	Counterfactual MA Policy Gradients, G. Farquhar, J. Foerster, N. Nardelli, S. Whiteson, T. Afouras (2018)
[ FNFplus-2017 ]	Stabilising experience replay for DMARL, G. Farquhar, J. Foerster, N. Nardelli, P. Kohli, P. Torr, S. Whiteson, T. Afouras (2017)
[ FSHplus-2019  ]	Bayesian action decoder for MADRL, E. Hughes, F. Song, J. Foerster (2019)
[ GAGplus-2018 ]	Learning policy representation in MAs, A. Grover, J. K. Gupta, M. Al-Shedivat (2018)
[ GD-2021 ]	MADRL: A survey, K. Diepold, S. Gronauer (2021)
[ GEK-2017 ]	Cooperative MA control using Deep-RL, J. K. Gupta, M. Egorov, M. Kochenderfer (2017)
[ GLP-2002 ]	Coordinated RL, C. Guestrin, M. Lagoudakis, R. Parr (2002)
[ GMM-2006 ]	HMARL, M. Ghavamzadeh, R. Makar, S. Mahadevan (2006)
[ GSE-2020 ]	Causal games and causal Nash equilibrium, H. Escalante, L. Sucar, M. Gonzalez-Soto (2020)
[ GSP-2021 ]	Causal MARL: Review and Open Problems, A. Pretorius, J. Shock, S. J. Grimbly (2021)
[ HBWR-2019 ]	MA hierarchical RL with dynamic termination, A. Rogers, D. Hajinezhad, M. Wooldridge, W. Boehmer (2019)
[ HGKD-2016 ]	Opponent modelling in MADRL, H. Daume, H. Hendrikx, J. B. Graber, K. Kwok (2016)
[ HLCplus-2021 ]	Off-belief Learning, A.  Lerer, B. Cui, D.  Wu, H.  Hu, J.  Foerster, L. Pineda, N. Brown (2021)
[ HMOM-2020 ]	Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients, A. Gruslys, D. Hennes, D. Morrill, E.  D. Guzman, J. B. Lespiau, J. Perolat, K. Tuyls, M. Lanctot, P. Parmas, R. Munos, S. Omidshafiei (2020)
[ HS-2016 ]	Deep-RL from self-play in imperfect-information games, D. Silver, J. Heinrich (2016)
[ HSS-2006 ]	Complex-valued RL, S. Shimada, T. Hamagami, T. Shibuya (2006)
[ HSSplus-2017 ]	A deep policy inference q-network for MAs, R. S. Sutton, T. Shann, Z. Hong (2017)
[ IS-2019 ]	Actor-attention-critic for MARL, F. Sha, S. Iqbal (2019)
[ ISPplus-2021 ]	Randomized Entity-wise Factorization for MARL, B. Peng, C. Schroeder, S. Iqbal (2021)
[ JBGplus-2022 ]	Distributed Cooperative MARL with Directed Coordination Graph, G. Jing, H. Bai, J. George (2022)
[ JCDM-2019 ]	Human-level performance in first-person multiplayer games with population-based deep reinforcement learning, I. Dunning, L. Marris, M. Jaderberg, W. M. Czarnecki (2019)
[ JCDplus-2019 ]	Human-level performance in 3d multiplayer games with population-based RL, I. Dunning, M. Jaderberg, W. M. Czarnecki (2019)
[ JDHL-2020 ]	Graph convolutional reinforcement learning, C. Dun, J. Jiang, T. Huang, Z. Lu (2020)
[ JLHplus-2018 ]	Intrinsic Social Motivation Via Causal Influence in MARL, A. Lazaridou, C. Gulcehre, D. Strouse, E. Hughes, J. Z. Leibo, N. Freitas, N. Jaques, P. A. Ortega (2018)
[ KL-2002 ]	Approximately Optimal Approximate RL, J. Langford, S. Kakade (2002)
[ KMP-2013 ]	QD-Learning: A collaborative distributed strategy for MARL through consensus + innovation, H. V. Poor, J. M. Moura, S. Kar (2013)
[ KMZ-2016 ]	Dual formulations for optimizing Dec-POMDPs Controllers, A. Kumar, H. Mostafa, S. Zilberstein (2016)
[ KSHH-2017 ]	Federated control with HMADRL, D. Hakkani-Tur, L. P. Heck, P. Shah, S. Kumar (2017)
[ KXLW-2017 ]	Revisiting the master-slave architecture in MADRL, B. Xin, F. Liu, X. Kong, Y. Wang (2017)
[ KZ-2010a ]	Anytime planning for De-POMDPs using expectation maximization, A. Kumar, S. Zilberstein (2010)
[ KZ-2010b ]	Point-based Backup for Dec-POMDPs, A. Kumar, S. Zilberstein (2010)
[ L-2001 ]	Value-function RL in Markov game, M. L. Littman (2001)
[ L-2016 ]	Efficient Bayesian Nonparametric Methods for Model-Free Reinforcement Learning in Centralized and Decentralized Sequential Environments, M. Liu (2016)
[ LA-2020 ]	Likelihood quantile networks for coordinating MARL, C. Amato, X. Lyu (2020)
[ LAAGH-2016 ]	Learning for Decentralized Control of MAS in Large, PO Stochastic Environments, C. Amato, E. P. Anesta, J. D. Griffith, J. P. How, M. Liu (2016)
[ LALCH-2015 ]	Stick-Breaking Policy Learning in Dec-POMDPs, C. Amato, J. P. How, L. Carin, M. Liu, X. Liao (2015)
[ LB-2018 ]	Structural causal bandits: where to intervene, E. Bareinboim, S. Lee (2018)
[ LFBRW-2019 ]	Stable Opponent Shaping in Differentiable Games, A. Letcher, D. Balduzzi, S. Whiteson, T. Rocktaschel (2019)
[ LKBC-2019 ]	A survey of learning in MA Environments: Dealing with Non-stationarity, E. M. Cote, M. Kaisers, P. H. Leal, T. Baarslag (2019)
[ LKPS-2019 ]	Learning Multi-level hierarchies with hindsight, A. Levy, G. D. Konidaris, K. Saenko, R. Platt (2019)
[ LKT-2019 ]	A Survey and Critique of MADRL, B. Kartal, M. Taylor, P. H. Leal (2019)
[ LL-2021 ]	MARL algorithm to solve a PO MA problem in disaster response, H. Lee, T. Lee (2021)
[ LLH-2019 ]	Emergent coordination through competition, G. Lever, N. Heess, S. Liu, T. Graepel (2019)
[ LMMN-2021 ]	Convex Q-Learning, F. Lu, G. Neu, P. Mehta, S. Meyn (2021)
[ LR-2000 ]	An algorithm for distributed RL in cooperative MAs, M. Lauer, M. Riedmiller (2000)
[ LR-2004 ]	RL for Stochastic Cooperative MAs, M. Lauer, M. Riedmiller (2004)
[ LSL-2016 ]	Distributed Cooperative Decision-Making in Multiarmed Bandits: Frequentist and Bayesian Algorithms, N. Leonard, P. Landgren, V. Srivastava (2016)
[ LSL-2020 ]	Distributed Cooperative Decision Making in Multi-agent Multi-armed Bandits, N. Leonard, P. Landgren, V. Srivastava (2020)
[ LTTCK-2021 ]	Information Directed Reward Learning for RL, A. Krause, D. Lindner, K. Ciosek, M. Turchetta, S. Tschiatschek (2021)
[ LVPplus-2021 ]	Lyapunov Exponents for Diversity in Differentiable Games, J. Lorraine, J. Parker, P. Vicol (2021)
[ LWCplus-2019 ]	Robust MARL via minimax deep deterministic policy gradient, F. Fang, H. Dong, S. Li, S. Russel, X. Cui, Y. I. Wu (2019)
[ LWTplus-2017 ]	MA AC for mixed cooperative-competitive environments, A. Tamar, I. Mordatch, J. Harb, R. Lowe, Y. I. Wu (2017)
[ LYCL-2017 ]	Coordinated MA imitation learning, H. M. Le, P. Carr, P. Lucey, Y. Yue (2017)
[ LYH-2018 ]	Primal-dual algorithm for distribute RL, D. Lee, H. Yoon, N. Hovakimyan (2018)
[ LYL-2020 ]	Learning to coordinate manipulation skills via skill behaviour diversification, J. J. Lim, J. Yang, Y. Lee (2020)
[ LYS-2020 ]	PIC: permutation invariant critic for MADRL, A. G. Schwing, I. J. Liu, R. A. Yeh (2020)
[ LZGplus-2017 ]	A unified game-theoretic approach to MARL, A. Gruslys, A. Lazaridou, D. Silver, J. Perolat, K. Tuyls, M. Lanctot, T. Graepel, V. Zambaldi (2017)
[ LZGplus-2020 ]	Evolutionary population curriculum for scaling MARL, A. Gupta, F. Fang, F. Wu, Q. Long, Z. Zhu (2020)
[ LZLplus-2017 ]	MARL in sequential social dilemmas, J. Z. Leibo, M. Lanctot, T. Graepel, V. Zambaldi (2017)
[ M-1961 ]	Step toward artificial intelligence, M. Minsky (1961)
[ MDSplus-2021 ]	Hindsight and Sequential Rationality of Correlated Play, D. Morrill, R. D&#x27;Orazio, R. Sarfati (2021)
[ MGHM-2017 ]	Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning, A. Maurer, E. M. E. Mhamdi, H. Hendrikx, R. Guerraoui (2017)
[ MGS-2021 ]	Neural Lyapunov Redesign, A. Mehrjou, B. Scholkopf, M. Ghavamzadeh (2021)
[ MI-2013 ]	Point based value iteration with optimal belief compression for Dec-POMDPs, C. L. Isbell, l. C. MacDermed (2013)
[ MMML-2005 ]	Distributed learning of MA causal models, B. Manderick, P. Leray, S. Maes, S. Meganck (2005)
[ MMP-2015 ]	Approximate Value Iteration with Temporally Extended Actions, D. Precup, S. Mannor, T. A. Mann (2015)
[ MPLplus-2020 ]	Fast Computation of Nash Equilibria in Imperfect Information Games, A. Gruslys, B. D. Vylder, D. Hennes, F. Timbers, J. B. Lespiau, J. Perolat, K. Tuyls, M. Lanctot, M. Rowland, R. Munos, S. Omidshafiei (2020)
[ MS-1998 ]	Macro-actions in RL: An empirical analysis, A. McGoven, R. S. Sutton (1998)
[ MS-2008 ]	Finite-Time Bounds for Fitted Value Iteration, C. Szepesvari, R. Munos (2008)
[ MSMplus-2021 ]	Tesseract: Tensorised Actors for MARL, A. Mahajan, L. Mao, M. Samvelyan (2021)
[ MTHplus-2017 ]	Diff-DAC: Distributed actor-critic for average multitask Deep-RL, A. Tukiainen, D. Hernandez, S. V. Macua (2017)
[ MV-2011 ]	Dec-POMDPs with sparse interactions, F. S. Melo, M. Veloso (2011)
[ MW-2020 ]	Feudal MADRL for traffic signal control, F. Wu, J. Martens (2020)
[ MWVplus-2021 ]	Counterfactual Credit Assignment in Model-free RL, F. Viola, T. Mesnard, T. Weber (2021)
[ MZMB-2020 ]	Information State Embedding in PO Cooperative MARL, E. Miehling, K. Zhang, T. Basar, W. Mao (2020)
[ NGLL-2018 ]	Data-efficient HRL, H. Lee, O. Nachum, S. Gu, S. Levine (2018)
[ NNN-2020 ]	Deep-RL for MAS: A Review of Challenges, Solutions and Applications, N. D. Nguyen, S. Nahavandi, T. T. Nguyen (2020)
[ NTYplus-2003 ]	Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings, D. V. Pynadath, M. Tambe, M. Yokoo, R. Nair, S. Marsella (2003)
[ NVTY-2005 ]	Networked Distributed POMDPs, M. Tambe, M. Yokoo, P. Varakantham, R. Nair (2005)
[ OA-2015 ]	A Coincise Introduction to Dec-POMDPs, C. Amato, F. A. Oliehoek (2015)
[ OAALPV-2017 ]	 Decentralized control of multi-robot POMDPs using belief space
macro-actions, A. Agha-Mohammadi, C. Amato, J. P. How, J. Vian, S. Liu, S. Omidshafiei (2017)
[ OH-2021 ]	A review of cooperative MADRL, A. Oroojlooy, D. Hajinezhad (2021)
[ OKLplus-2019 ]	Learning to teach in cooperative MARL, C. Amato, D. Kim, J. P. How, M. Liu, S. Omidshafiei (2019)
[ OKMK-2000 ]	Learning to cooperate via policy search, K. E. Kim, L. P. Kaelbling, L. Peshkin, N. Meuleau (2000)
[ OKV-2008 ]	The cross-entropy method for policy search in Dec-POMDPs, F. A. Oliehoek, F. P. Kooij, N. Vlassis (2008)
[ OPAHV-2017 ]	Deep Decentralized Multi-task Multi-agent RL under PO, C. Amato, J. P. How, J. Pazis, J. Vian, S. Omidshafiei (2017)
[ OSV-2008 ]	Optimal and Approximate Q-value functions for Dec-POMDPs, F. A. Oliehoek, M. T. J. Spaan, N. Vlassis (2008)
[ OSW-2015 ]	Influence-optimistic local values for MA planning, F. A. Oliehoek, M. T. J. Spaan, S. J. Witwicki (2015)
[ OSWV-2008 ]	Exploiting locality of interaction in factored Dec-POMDPs, F. A. Oliehoek, M. T. J. Spaan, N. Vlassis, S. Whiteson (2008)
[ OV-2006 ]	A hierarchical model for decentralized fighting of large scale urban fires, A. Visser, F. A. Oliehoek (2006)
[ OWS-2009 ]	Lossless clustering of histories in Dec-POMDPs, F. A. Oliehoek, M. T. J. Spaan, S. Whiteson (2009)
[ P-2022 ]	Configurable Dec-POMDPs, R. Poiani (2022)
[ PCA-2021 ]	Agent Modelling under PO for Deep-RL, F. Christianos, G. Papoudakis, S. V. Albrech (2021)
[ PCBplus-2020 ]	A game-theoretic analysis of networked system control for common-pool resource management using MARL, A. Laterre, A. Pretorius, E. Biljon, J. Plessis, J. Shock, K. Beguir, S. Cameron, S. Mawjee, T. Makkink (2020)
[ PCRA-2019 ]	Dealing with Non-Stationarity in MADRL, A. Rahman, F. Christianos, G. Papoudakis, S. V. Albrech (2019)
[ PCSA-2021 ]	Comparative Evaluation of Cooperative MADRL Algorithms, F. Christianos, G. Papoudakis, L. Schafer, S. V. Albrech (2021)
[ PDSG-2017 ]	Robust adversarial RL, A. Gupta, J. Davidson, L. Pinto, R. Sukthankar (2017)
[ PMLOR-2021 ]	From Poincare Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization, B. D. Vylder, D. Balduzzi, G. Piliouras, J. B. Lespiau, J. Perolat, K. Tuyls, M. Lanctot, M. Rowland, P. A. Ortega, R. Munos, S. Omidshafiei (2021)
[ PP-2010 ]	A distributed actor-critic algorithm and application to mobile sensor network coordination problems, I. C. Paschalidis, P. Pennesi (2010)
[ PP-2013 ]	Periodic FSC for efficient Dec-POMDPs, J. Pajarinen, J. Peltonen (2011)
[ PPP-2018 ]	Actor-critic fictitious play in simultaneous move multistage games, B. Piot, J. Perolat, O. Pietquin (2018)
[ PR-1998 ]	RL with hierarchies of machines, R. Parr, S. Russel (1998)
[ PRBplus-2021 ]	VAST: Value Function Factorization with Variable Agent Sub-Teams, F. Ritz, L. Belzner, T. Phan (2021)
[ PRPC-2013 ]	Safe Policy Iteration, A. Pecorino, D. Calandriello, M. Pirotta, M. Restelli (2013)
[ PST-2019 ]	Negative update intervals in MADRL, G. Palmer, K. Tuyls, R. Savani (2019)
[ PSTQ-2021 ]	Hierarchical Reinforcement Learning: A Comprehensive Survey, A. Tan, B. Subagdja, C. Quek, S. Pateria (2021)
[ PTBS-2018 ]	Lenient MADRL, D. Blombergen, G. Palmer, K. Tuyls, R. Savani (2018)
[ R-2021 ]	Challenges and opportunities in MARL, G. Ramponi (2021)
[ RDSF-2018 ]	Modelling others using oneself in MARL, A. Szlam, E. Denton, R. Fergus, R. Raileanu (2018)
[ RGS-2003 ]	The complexity of MAS: the price of silence, C. V. Goldman, J. S. Rosenschein, Z. Rabinovich (2003)
[ RH-2021 ]	Cooperative Equilibrium: A Solution Predicting Cooperative Play, J. Halpern, N. Rong (2021)
[ RKR-2019 ]	Decentralized Cooperative Stochastic Bandits, D. Rubio, P. Rebeschini, V. Kanade (2019)
[ RN-2009 ]	Artificial Intelligence: A modern approach , P. Norvig, S. Russel (2009)
[ ROHplus-2021 ]	 Temporal Difference and Return Optimism in Cooperative
MARL, D. Hennes, M. Rowland, S. Omidshafiei (1900)
[ ROHplus-2021 ]	Temporal Difference and Return Optimism in Cooperative MARL, A. Jaegle, D. Hennes, J. Perolat, K. Tuyls, M. Rowland, P. Muller, S. Omidshafiei, W. Dabney (2021)
[ ROTplus-2020 ]	MA Evaluation under Incomplete Information, G. Piliouras, J. Perolat, K. Tuyls, M. Rowland, M.Valko, R. Munos, S. Omidshafiei (2020)
[ RR-2021 ]	Newton Optimization on Helmholtz Decomposition for Continuous Games, G. Ramponi, M. Restelli (2021)
[ RSHplus-2018 ]	QMIX: Monotonic value function factorization for Deep-MARL, C. Farquhar, J.  Foerster, M. Samvelyan, S. Whiteson, T. Rashid (2018)
[ RSP-2018 ]	 MA actor-critic with generative cooperative
policy network, H. Ryu, H. Shin, J. Park (2018)
[ S-2021 ]	Solving Common-Payoff Games with Approximate Policy Iteration, S. Sokota (2021)
[ SARplus-2020 ]	QTRAN++: Improved Value Transformation for Cooperative MARL, K. Son, S. Ahilan (2020)
[ SB-2017 ]	Equilibrium Propagation: Bridging the gap between energy-based models and backpropagation, B. Scellier, Y. Banjo (2017)
[ SC-2006 ]	Point-based dynamic programming for Dec-POMDPs, D. Szer, F. Charpillet (2006)
[ SERS-2021 ]	Agent-Centric Representations for MARL, A. Raichuk, L. Espeholt, T. Salimans, W. Shang (2021)
[ SFZplus-2021 ]	Survey of recent MARL utilizing centralized training, E. G. Zaroukian, P. Sharma, R. Fernandez (2021)
[ SKKplus-2019 ]	QTRAN: Learning to factorize with transformation for cooperative MARL, D. Kim, K. Son, W. J. Kang (2019)
[ SLGplus-2018 ]	Value-decomposition networks for cooperative MARL based on team reward, A. Gruslys, G. Lever, P. Sunehag (2018)
[ SLL-2021 ]	DFAC: Factorizing the Value Function via Quantile Mixture for MA Distributional Q-Learning, C. Lee, W. Sun (2021)
[ SLTplus-2021 ]	Solving Common-Payoff Games with Approximate Policy Iteration, E. Lockhart, F. Timbers, S. Sokota (2021)
[ SMW-2016 ]	An emphatic approach to the problem of off-policy TD learning, A. R. Mahmood, M. White, R. S. Sutton (2016)
[ SOA-2011 ]	Scaling up optimal heuristic search in Dec-POMDPs via incremental expansion, C. Amato, F. A. Oliehoek, M. T. J. Spaan (2011)
[ SPS-1999 ]	Between MDPs and Semi-MDPs, D. Precup, R. S. Sutton, S. Singh (1999)
[ SWYY-2019 ]	Solving Discounted Stochastic Two Player Games with Near-Optimal Time and Sample Complexity, A. Sidford, L. Yang, M. Wang, Y. Ye (2019)
[ SWZ-2019 ]	Convergence of Multi-Agent Learning with a Finite Step Size in General-Sum Games, C. Zhang, T. Wang, X. Song (2019)
[ SYZplus-2020 ]	A MA off-policy actor-critic algorithm for distributed reinforcement learning, I. J. Liu, K. Zhang, T. Basar, W. Suttle, Z. Yang (2020)
[ SZ-2007a ]	Memory-bounded dynamic programming for Dec-POMDPs, S. Seuken, S. Zilberstein (2007)
[ SZ-2007b ]	Improved memory-bounded dynamic programming for Dec-POMDPs, S. Seuken, S. Zilberstein (2007)
[ T-1993 ]	MARL: Independent vs. cooperative agents, M. Tan (1993)
[ THLplus-2018 ]	HMADRL, H. Tang, J. Hao, T. Lv, Y. Zheng (2018)
[ TKT-2019 ]	Robust Model-free RL with Multi-objective Bayesian Optimization, A. Krause, M. Turchetta, S. Trimpe (2019)
[ TSMplus-2019 ]	Relational forward models for MARL, A. Tacchetti, H. F. Song, P. Mediano, V. Zambaldi (2019)
[ TWGplus-2019 ]	A Regularized Opponent Model with Maximum Entropy Objective, Y. Wen, Z. Gong, Z. Tian (2019)
[ VBCM-2019 ]	Grandmaster level in StarCraft II using MARL, I. Babuschkin, M. Mathieu, O. Vinyals, W. M. Czarnecki (2019)
[ VVMplus-2020 ]	Learning to communicate using counterfactual reasoning, A. Vanneste, K. Mets, S. Vanneste (2020)
[ VWLL-2019 ]	Option as responses: grounding behavioural hierarchies in MARL, A. S. Vezhevets, J. Z. Leibo, R. Leblond, Y. I. Wu (2019)
[ W-2020 ]	Modelling Mutual Influence in MARL, Y. Wen (2020)
[ WCC-2021 ]	A Distributional Perspective on Value Function Factorization Methods for MARL, L. Cheng, L. Chun, S. Wei (2021)
[ WDLZ-2020 ]	ROMA: MARL with emergent-roles, C. Zhang, H. Dong, T. Wang, V. Lesser (2020)
[ WEEplus-2020 ]	Too many cooks: Coordinating MA collaboration through inverse planning, D. C. Parkes, J. A. Evans, J. B. Tenenbaum, M. Kleiman-Weiner, R. E. Wang, S. A. Wu (2020)
[ WHFplus-2019 ]	Evolving intrinsic motivations for altruistic behaviours, C. Fernando, E. Hughes, J. Wang, J. Z. Leibo (2019)
[ WRDplus-2018 ]	Variance reduction for policy gradient with action-dependent factorized baselines, A. Rajeswaran, C. Wu, Y. Duan (2018)
[ WRH-2021 ]	Towards Understanding Cooperative MA Q-Learning with Value Factorization, B. Han, C. Zhang, J. Wang, Z. Ren (2021)
[ WRHplus-2021 ]	Towards Understanding Cooperative MA Q-Learning with Value Factorization, B. Han, J. Wang, Z. Ren (2021)
[ WRLplus-2021 ]	QPLEX: Duplex Duelling MA Q-Learning, J. Wang, T. Liu, Z. Ren (2021)
[ WS-2002 ]	RL to Play an Optimal NE in Team Markov Games, T. Sandholm, X.  Wang (2002)
[ WWW-2020 ]	Influence-based MA exploration, C. Zhang, J. Wang, T. Wang, Y. I. Wu (2020)
[ WWZZ-2020 ]	Learning nearly decomposable value functions via communication minimization, C. Zhang, C. Zheng, J. Wang, T. Wang (2020)
[ WYLplus-2018 ]	Probabilistic Recursive Reasoning for MARL, J. Wang, R. Luo, W. Pan, Y. Wen, Y. Yang (2018)
[ WYWH-2019 ]	MARL via Double Averaging Primal-Dual Optimization, H. Wai, M. Hong, Z. Wang, Z. Yang (2019)
[ WZC-2010 ]	Point-based policy generation for Dec-POMDPs, F. Wu, S. Zilberstein, X. Chen (2010)
[ YAY-2019 ]	Learning team-optimality for decentralized stochastic control and dynamic games, B. Yongacoglu, G. Arslan, S. Yuksel (2019)
[ YAY-2021 ]	Decentralized Learning for Optimality in Stochastic Dynamic Teams and Games with Local Control and Global State Information, B. Yongacoglu, G. Arslan, S. Yuksel (2021)
[ YBZ-2020 ]	Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery, H. Zha, I. Borovikov, J. Yang (2020)
[ YLLplus-2018 ]	Mean-field MARL, J. Wang, M. Li, R. Luo, Y. Yang (2018)
[ YMLplus-2021 ]	Believe what you see: implicit constraint approach for offline MARL, C. Li, X. Ma, Y. Yang (2021)
[ YNIplus-2020 ]	CM3: Cooperative multi-goal multi-stage MARL, A. Nakhaei, J. Yang (2020)
[ YSE-2019 ]	MA adversarial inverse RL, J. Song, L. Yu, S. Ermon (2019)
[ YVVplus-2021 ]	The Surprising Effectiveness of PPO in Cooperative MA Games, A. Velu, C. Yu, E. Vinitsky (2021)
[ YW-2021 ]	An overview on MARL from Game Theoretical Perspective, J. Wang, Y. Yang (2021)
[ ZB-2019 ]	MAMPS: Safe MARL via model predictive shielding, O. Bastani, W. Zhang (2019)
[ ZBS-2020 ]	MA Safe Planning with Gaussian Processes, D. Sadigh, E. Biyik, Z. Zhu (2020)
[ ZCWplus-2019 ]	Factorized Q-Learning for Large-Scale MAS, M. Zhou, Y. Chen, Y. Wen (2019)
[ ZGSW-2021 ]	Learning Fair Policies in Decentralized Cooperative MARL, C. Glanois, M.  Zimmer, P. Weng, U. Siddique (2021)
[ ZL-2010 ]	MARL with Policy Prediction, C. Zhang, V. Lesser (2010)
[ ZLWplus-2021 ]	FOP: Factorizing Optimal Joint Policy of Maximum-Entropy MARL, C. Weng, T. Zhang, Y. Li (2021)
[ ZLWplus-2021 ]	FOP: Factorizing optimal Joint Policies of Maximum-Entropy MARL, C. Wang, T. Zhang, Y. Li (2021)
[ ZMHplus-2018 ]	Weighted double MADRL in stochastic cooperative environment, J. Hao, Y. Zheng, Z. Meng (2018)
[ ZPHplus-2019 ]	Malthusian RL, E. Hughes, I. Dunning, J. Perolat, J. Z. Leibo, T. Graepel (2019)
[ ZRL-2021 ]	Gradient play in stochastic games: stationary points, convergence, and sample complexity, N. Li, R. Zhang, Z. Ren (2021)
[ ZYB-2018 ]	Networked MARL in continuous spaces, K. Zhang, T. Basar, Z. Yang (2018)
[ ZYB-2019 ]	MARL: a selective overview of theories and algorithms, K. Zhang, T. Basar, Z. Yang (2019)
[ ZYLZB-2018 ]	Fully decentralized multi-agent reinforcement learning with networked agents, H. Liu, K. Zhang, T. Basar, T. Zhang, Z. Yang (2018)
[ ZZ-2019 ]	 Distributed off-policy actor-critic RL with policy consensus., M. M. Zavlanos, Y. Zhang (2019)</span></span></p></div><div id="https://www.notion.so/0f55759364c145a0afa2245d44c975c7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>
  <footer class="Footer">
  <div>&copy; DignoLog 2019-2021</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>